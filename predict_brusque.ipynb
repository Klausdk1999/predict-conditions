{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ./dataset/data_log_0.csv\n",
      "Successfully loaded ./dataset/data_log_1.csv\n",
      "Successfully loaded ./dataset/data_log_2.csv\n",
      "Successfully loaded ./dataset/data_log_3.csv\n",
      "Successfully loaded ./dataset/data_log_4.csv\n",
      "Successfully loaded ./dataset/data_log_5.csv\n",
      "Successfully loaded ./dataset/data_log_6.csv\n",
      "Successfully loaded ./dataset/data_log_7.csv\n",
      "Successfully loaded ./dataset/data_log_8.csv\n",
      "Successfully loaded ./dataset/data_log_9.csv\n",
      "Successfully loaded ./dataset/data_log_10.csv\n",
      "Successfully loaded ./dataset/data_log_11.csv\n",
      "Successfully loaded ./dataset/data_log_12.csv\n",
      "Successfully loaded ./dataset/data_log_13.csv\n",
      "Successfully loaded ./dataset/data_log_14.csv\n",
      "Successfully loaded ./dataset/data_log_15.csv\n",
      "Successfully loaded ./dataset/data_log_16.csv\n",
      "Successfully loaded ./dataset/data_log_17.csv\n",
      "Successfully loaded ./dataset/data_log_18.csv\n",
      "Successfully loaded ./dataset/data_log_19.csv\n",
      "Successfully loaded ./dataset/data_log_20.csv\n",
      "Successfully loaded ./dataset/data_log_21.csv\n",
      "Successfully loaded ./dataset/data_log_22.csv\n",
      "Successfully loaded ./dataset/data_log_23.csv\n",
      "Successfully loaded ./dataset/data_log_24.csv\n",
      "Successfully loaded ./dataset/data_log_25.csv\n",
      "Successfully loaded ./dataset/data_log_26.csv\n",
      "Successfully loaded ./dataset/data_log_27.csv\n",
      "Successfully loaded ./dataset/data_log_28.csv\n",
      "Successfully loaded ./dataset/data_log_29.csv\n",
      "Successfully loaded ./dataset/data_log_30.csv\n",
      "Successfully loaded ./dataset/data_log_31.csv\n",
      "Successfully loaded ./dataset/data_log_32.csv\n",
      "Successfully loaded ./dataset/data_log_33.csv\n",
      "Successfully loaded ./dataset/data_log_34.csv\n",
      "Successfully loaded ./dataset/data_log_35.csv\n",
      "Successfully loaded ./dataset/data_log_36.csv\n",
      "Successfully loaded ./dataset/data_log_37.csv\n",
      "Successfully loaded ./dataset/data_log_38.csv\n",
      "Successfully loaded ./dataset/data_log_39.csv\n",
      "Successfully loaded ./dataset/data_log_40.csv\n",
      "Successfully loaded ./dataset/data_log_41.csv\n",
      "Successfully loaded ./dataset/data_log_42.csv\n",
      "Successfully loaded ./dataset/data_log_43.csv\n",
      "Successfully loaded ./dataset/data_log_44.csv\n",
      "Successfully loaded ./dataset/data_log_45.csv\n",
      "Successfully loaded ./dataset/data_log_46.csv\n",
      "Successfully loaded ./dataset/data_log_47.csv\n",
      "Successfully loaded ./dataset/data_log_48.csv\n",
      "Successfully loaded ./dataset/data_log_49.csv\n",
      "Successfully loaded ./dataset/data_log_50.csv\n",
      "Successfully loaded ./dataset/data_log_51.csv\n",
      "Successfully loaded ./dataset/data_log_52.csv\n",
      "Successfully loaded ./dataset/data_log_53.csv\n",
      "Successfully loaded ./dataset/data_log_54.csv\n",
      "Successfully loaded ./dataset/data_log_55.csv\n",
      "Successfully loaded ./dataset/data_log_56.csv\n",
      "Successfully loaded ./dataset/data_log_57.csv\n",
      "Successfully loaded ./dataset/data_log_58.csv\n",
      "Successfully loaded ./dataset/data_log_59.csv\n",
      "Successfully loaded ./dataset/data_log_60.csv\n",
      "Successfully loaded ./dataset/data_log_61.csv\n",
      "Successfully loaded ./dataset/data_log_62.csv\n",
      "Successfully loaded ./dataset/data_log_63.csv\n",
      "Successfully loaded ./dataset/data_log_64.csv\n",
      "Successfully loaded ./dataset/data_log_65.csv\n",
      "Successfully loaded ./dataset/data_log_66.csv\n",
      "Successfully loaded ./dataset/data_log_67.csv\n",
      "Successfully loaded ./dataset/data_log_68.csv\n",
      "Successfully loaded ./dataset/data_log_69.csv\n",
      "Successfully loaded ./dataset/data_log_70.csv\n",
      "Successfully loaded ./dataset/data_log_71.csv\n",
      "Successfully loaded ./dataset/data_log_72.csv\n",
      "Successfully loaded ./dataset/data_log_73.csv\n",
      "Successfully loaded ./dataset/data_log_74.csv\n",
      "Successfully loaded ./dataset/data_log_75.csv\n",
      "Successfully loaded ./dataset/data_log_76.csv\n",
      "Successfully loaded ./dataset/data_log_77.csv\n",
      "Successfully loaded ./dataset/data_log_78.csv\n",
      "Successfully loaded ./dataset/data_log_79.csv\n",
      "Successfully loaded ./dataset/data_log_80.csv\n",
      "Successfully loaded ./dataset/data_log_81.csv\n",
      "Successfully loaded ./dataset/data_log_82.csv\n",
      "Successfully loaded ./dataset/data_log_83.csv\n",
      "Successfully loaded ./dataset/data_log_84.csv\n",
      "Successfully loaded ./dataset/data_log_85.csv\n",
      "Successfully loaded ./dataset/data_log_86.csv\n",
      "Successfully loaded ./dataset/data_log_87.csv\n",
      "Successfully loaded ./dataset/data_log_88.csv\n",
      "Successfully loaded ./dataset/data_log_89.csv\n",
      "Successfully loaded ./dataset/data_log_90.csv\n",
      "Successfully loaded ./dataset/data_log_91.csv\n",
      "Successfully loaded ./dataset/data_log_92.csv\n",
      "Successfully loaded ./dataset/data_log_93.csv\n",
      "Successfully loaded ./dataset/data_log_94.csv\n",
      "Successfully loaded ./dataset/data_log_95.csv\n",
      "Successfully loaded ./dataset/data_log_96.csv\n",
      "Successfully loaded ./dataset/data_log_97.csv\n",
      "Successfully loaded ./dataset/data_log_98.csv\n",
      "Successfully loaded ./dataset/data_log_99.csv\n",
      "Successfully loaded ./dataset/data_log_100.csv\n",
      "Successfully loaded ./dataset/data_log_101.csv\n",
      "Successfully loaded ./dataset/data_log_102.csv\n",
      "Successfully loaded ./dataset/data_log_103.csv\n",
      "Successfully loaded ./dataset/data_log_104.csv\n",
      "Successfully loaded ./dataset/data_log_105.csv\n",
      "Successfully loaded ./dataset/data_log_106.csv\n",
      "Successfully loaded ./dataset/data_log_107.csv\n",
      "Successfully loaded ./dataset/data_log_108.csv\n",
      "Successfully loaded ./dataset/data_log_109.csv\n",
      "Successfully loaded ./dataset/data_log_110.csv\n",
      "Successfully loaded ./dataset/data_log_111.csv\n",
      "Successfully loaded ./dataset/data_log_112.csv\n",
      "Successfully loaded ./dataset/data_log_113.csv\n",
      "Successfully loaded ./dataset/data_log_114.csv\n",
      "Successfully loaded ./dataset/data_log_115.csv\n",
      "Successfully loaded ./dataset/data_log_116.csv\n",
      "Successfully loaded ./dataset/data_log_117.csv\n",
      "Successfully loaded ./dataset/data_log_118.csv\n",
      "Successfully loaded ./dataset/data_log_119.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Variable to set how many files to read\n",
    "num_files_to_read = 120  # Set the number of files to read (adjust as needed)\n",
    "file_names = [f\"./dataset/data_log_{i}.csv\" for i in range(num_files_to_read)]\n",
    "\n",
    "# Columns definition\n",
    "columns = [\n",
    "    \"nome\", \"codigo\", \"tipo\", \"last_read\", \"nivel_rio\", \"nivel_rio_historico\",\n",
    "    \"chuva_001h\", \"chuva_003h\", \"chuva_006h\", \"chuva_012h\", \"chuva_024h\",\n",
    "    \"chuva_048h\", \"chuva_072h\", \"chuva_096h\", \"chuva_120h\", \"chuva_144h\",\n",
    "    \"chuva_168h\", \"temp_atual\", \"temp_sens\", \"umidade\", \"vel_vento\",\n",
    "    \"dir_vento\", \"pres_atmos\", \"nivel_montante\", \"nivel_jusante\",\n",
    "    \"porc_reservatorio\", \"comp_abertas\", \"comp_fechadas\", \"__typename\",\n",
    "    \"localizacao.lat\", \"localizacao.lng\", \"localizacao.__typename\", \"timestamp\"\n",
    "]\n",
    "\n",
    "# Load CSV files with error handling\n",
    "dfs = []\n",
    "for file in file_names:\n",
    "    try:\n",
    "        df = pd.read_csv(file, names=columns, header=0)  # Skip the header row in all files\n",
    "        dfs.append(df)\n",
    "        print(f\"Successfully loaded {file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file} not found.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Unable to parse {file}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "if len(dfs) == 0:\n",
    "    raise Exception(\"No valid files were loaded. Please check the file paths or file content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames\n",
    "data = pd.concat(dfs)\n",
    "\n",
    "# Filter data for the selected stations\n",
    "stations = ['DCSC Brusque', 'DCSC Vidal Ramos']\n",
    "filtered_data = data[data['nome'].isin(stations)]\n",
    "\n",
    "# Pivot data to have separate columns for each station's data\n",
    "pivoted_data = filtered_data.pivot_table(index=\"timestamp\", columns=\"nome\", values=[\"nivel_rio\", \"chuva_024h\"])\n",
    "pivoted_data.columns = ['nivel_rio_brusque', 'nivel_rio_vidal_ramos', 'chuva_024h_brusque', 'chuva_024h_vidal_ramos']\n",
    "pivoted_data.reset_index(inplace=True)\n",
    "pivoted_data.sort_values(by=\"timestamp\", inplace=True)\n",
    "\n",
    "# Create lagged features\n",
    "pivoted_data['nivel_rio_brusque_lag1'] = pivoted_data['nivel_rio_brusque'].shift(1)\n",
    "pivoted_data['nivel_rio_vidal_ramos_lag1'] = pivoted_data['nivel_rio_vidal_ramos'].shift(1)\n",
    "pivoted_data['chuva_024h_brusque_lag1'] = pivoted_data['chuva_024h_brusque'].shift(1)\n",
    "pivoted_data['chuva_024h_vidal_ramos_lag1'] = pivoted_data['chuva_024h_vidal_ramos'].shift(1)\n",
    "\n",
    "# Create the target variable (next day's river level in Brusque)\n",
    "pivoted_data['nivel_rio_brusque_next_day'] = pivoted_data['nivel_rio_brusque'].shift(-1)\n",
    "\n",
    "# Drop missing values\n",
    "pivoted_data = pivoted_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = pivoted_data[['nivel_rio_brusque_lag1', 'nivel_rio_vidal_ramos_lag1', 'chuva_024h_brusque_lag1', 'chuva_024h_vidal_ramos_lag1']]\n",
    "y = pivoted_data['nivel_rio_brusque_next_day']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5044\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Regressor\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last 5 Predictions:\n",
      "        Actual  Predicted\n",
      "17227    0.00       0.00\n",
      "21251    0.00       0.00\n",
      "12268    0.50       0.50\n",
      "18517   12.42      12.42\n",
      "15424    0.38       0.38\n"
     ]
    }
   ],
   "source": [
    "# Display last 5 predictions for validation\n",
    "comparison = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).tail(5)\n",
    "print(\"\\nLast 5 Predictions:\\n\", comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Klaus\\AppData\\Local\\Temp\\ipykernel_15112\\2874394264.py:10: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Plot actual vs. predicted\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.title(\"Actual vs. Predicted River Levels (Brusque)\")\n",
    "plt.xlabel(\"Test Samples\")\n",
    "plt.ylabel(\"River Level (m)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Prediction Function\n",
    "def predict_next_day_river_level(latest_data=None):\n",
    "    \"\"\"\n",
    "    Predict the next day's river level for Brusque.\n",
    "    \n",
    "    Parameters:\n",
    "    latest_data (list or None): If None, uses the last row of the test set.\n",
    "                                Otherwise, provide a list [nivel_rio_brusque_lag1, nivel_rio_vidal_ramos_lag1, chuva_024h_brusque_lag1, chuva_024h_vidal_ramos_lag1].\n",
    "    \n",
    "    Returns:\n",
    "    float: Predicted river level for the next day.\n",
    "    \"\"\"\n",
    "    if latest_data is None:\n",
    "        latest_data = X_test.iloc[-1].values\n",
    "        print(\"Using the last test row for prediction:\", latest_data)\n",
    "    else:\n",
    "        print(\"Using provided data for prediction:\", latest_data)\n",
    "\n",
    "    latest_data_df = pd.DataFrame([latest_data], columns=X.columns)\n",
    "    return model.predict(latest_data_df)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the last test row for prediction: [0.38       0.02       1.28999996 1.64999998]\n",
      "Predicted river level for the next day in Brusque: 0.38\n"
     ]
    }
   ],
   "source": [
    "# Example: Predict using the last test set row\n",
    "predicted_river_level = predict_next_day_river_level()\n",
    "print(f\"Predicted river level for the next day in Brusque: {predicted_river_level:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
